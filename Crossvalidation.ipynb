{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from scipy.stats import norm\n",
    "\n",
    "from random import random\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "from sklearn import preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/Users/normalazo/Desktop/env1/Entrega6_CrossValidation/regLinPoli2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>X9</th>\n",
       "      <th>X10</th>\n",
       "      <th>...</th>\n",
       "      <th>X30</th>\n",
       "      <th>X31</th>\n",
       "      <th>X32</th>\n",
       "      <th>X33</th>\n",
       "      <th>X34</th>\n",
       "      <th>X35</th>\n",
       "      <th>X36</th>\n",
       "      <th>X37</th>\n",
       "      <th>X38</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1029.000000</td>\n",
       "      <td>1029.000000</td>\n",
       "      <td>1029.000000</td>\n",
       "      <td>1029.000000</td>\n",
       "      <td>1029.000000</td>\n",
       "      <td>1029.000000</td>\n",
       "      <td>1029.00000</td>\n",
       "      <td>1029.000000</td>\n",
       "      <td>1029.000000</td>\n",
       "      <td>1029.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1029.000000</td>\n",
       "      <td>1029.000000</td>\n",
       "      <td>1029.000000</td>\n",
       "      <td>1029.000000</td>\n",
       "      <td>1029.000000</td>\n",
       "      <td>1029.000000</td>\n",
       "      <td>1029.000000</td>\n",
       "      <td>1029.000000</td>\n",
       "      <td>1029.000000</td>\n",
       "      <td>1.029000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-12.492680</td>\n",
       "      <td>327718.372035</td>\n",
       "      <td>2.565422</td>\n",
       "      <td>-0.018122</td>\n",
       "      <td>1.235180</td>\n",
       "      <td>-153.782162</td>\n",
       "      <td>-23648.95337</td>\n",
       "      <td>-2.565422</td>\n",
       "      <td>0.018122</td>\n",
       "      <td>-4.108330</td>\n",
       "      <td>...</td>\n",
       "      <td>5.612245</td>\n",
       "      <td>100.650146</td>\n",
       "      <td>-29.719145</td>\n",
       "      <td>-136.164237</td>\n",
       "      <td>-101.649174</td>\n",
       "      <td>-34.310010</td>\n",
       "      <td>-7.965986</td>\n",
       "      <td>-37.117590</td>\n",
       "      <td>2.499514</td>\n",
       "      <td>-8.035132e+18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>572.608894</td>\n",
       "      <td>298395.344783</td>\n",
       "      <td>0.424105</td>\n",
       "      <td>0.715733</td>\n",
       "      <td>58.709228</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.424105</td>\n",
       "      <td>0.715733</td>\n",
       "      <td>584.648570</td>\n",
       "      <td>...</td>\n",
       "      <td>239.009325</td>\n",
       "      <td>2862.331915</td>\n",
       "      <td>4489.803784</td>\n",
       "      <td>2641.944700</td>\n",
       "      <td>5153.481524</td>\n",
       "      <td>2569.220416</td>\n",
       "      <td>2933.878621</td>\n",
       "      <td>1504.903744</td>\n",
       "      <td>102.994972</td>\n",
       "      <td>2.593868e+20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-992.795572</td>\n",
       "      <td>1.098476</td>\n",
       "      <td>0.020395</td>\n",
       "      <td>-0.999991</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>-153.782162</td>\n",
       "      <td>-23648.95337</td>\n",
       "      <td>-2.999260</td>\n",
       "      <td>-0.999992</td>\n",
       "      <td>-996.652132</td>\n",
       "      <td>...</td>\n",
       "      <td>-410.000000</td>\n",
       "      <td>-4857.000000</td>\n",
       "      <td>-7715.000000</td>\n",
       "      <td>-4596.000000</td>\n",
       "      <td>-8898.000000</td>\n",
       "      <td>-4383.000000</td>\n",
       "      <td>-5133.000000</td>\n",
       "      <td>-2565.000000</td>\n",
       "      <td>-175.000000</td>\n",
       "      <td>-9.506460e+20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-499.000060</td>\n",
       "      <td>59791.243170</td>\n",
       "      <td>2.388319</td>\n",
       "      <td>-0.760277</td>\n",
       "      <td>-51.000000</td>\n",
       "      <td>-153.782162</td>\n",
       "      <td>-23648.95337</td>\n",
       "      <td>-2.869121</td>\n",
       "      <td>-0.714801</td>\n",
       "      <td>-511.919891</td>\n",
       "      <td>...</td>\n",
       "      <td>-189.000000</td>\n",
       "      <td>-2457.000000</td>\n",
       "      <td>-4063.000000</td>\n",
       "      <td>-2438.000000</td>\n",
       "      <td>-4780.000000</td>\n",
       "      <td>-2311.000000</td>\n",
       "      <td>-2520.000000</td>\n",
       "      <td>-1411.000000</td>\n",
       "      <td>-85.000000</td>\n",
       "      <td>-7.703790e+18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-27.174576</td>\n",
       "      <td>241226.533900</td>\n",
       "      <td>2.691213</td>\n",
       "      <td>-0.029038</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>-153.782162</td>\n",
       "      <td>-23648.95337</td>\n",
       "      <td>-2.691213</td>\n",
       "      <td>0.029038</td>\n",
       "      <td>-6.507760</td>\n",
       "      <td>...</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>123.000000</td>\n",
       "      <td>129.000000</td>\n",
       "      <td>-196.000000</td>\n",
       "      <td>-13.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>-35.000000</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>-1.094503e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>477.216100</td>\n",
       "      <td>547320.664300</td>\n",
       "      <td>2.869121</td>\n",
       "      <td>0.714801</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>-153.782162</td>\n",
       "      <td>-23648.95337</td>\n",
       "      <td>-2.388319</td>\n",
       "      <td>0.760277</td>\n",
       "      <td>516.046297</td>\n",
       "      <td>...</td>\n",
       "      <td>214.000000</td>\n",
       "      <td>2654.000000</td>\n",
       "      <td>3890.000000</td>\n",
       "      <td>2138.000000</td>\n",
       "      <td>4195.000000</td>\n",
       "      <td>2165.000000</td>\n",
       "      <td>2521.000000</td>\n",
       "      <td>1227.000000</td>\n",
       "      <td>94.000000</td>\n",
       "      <td>5.636450e+18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>998.297367</td>\n",
       "      <td>996597.632200</td>\n",
       "      <td>2.999260</td>\n",
       "      <td>0.999992</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>-153.782162</td>\n",
       "      <td>-23648.95337</td>\n",
       "      <td>-0.020395</td>\n",
       "      <td>0.999991</td>\n",
       "      <td>1000.933758</td>\n",
       "      <td>...</td>\n",
       "      <td>410.000000</td>\n",
       "      <td>4862.000000</td>\n",
       "      <td>7752.000000</td>\n",
       "      <td>4577.000000</td>\n",
       "      <td>8866.000000</td>\n",
       "      <td>4384.000000</td>\n",
       "      <td>5123.000000</td>\n",
       "      <td>2568.000000</td>\n",
       "      <td>175.000000</td>\n",
       "      <td>9.881420e+20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                X1             X2           X3           X4           X5  \\\n",
       "count  1029.000000    1029.000000  1029.000000  1029.000000  1029.000000   \n",
       "mean    -12.492680  327718.372035     2.565422    -0.018122     1.235180   \n",
       "std     572.608894  298395.344783     0.424105     0.715733    58.709228   \n",
       "min    -992.795572       1.098476     0.020395    -0.999991  -100.000000   \n",
       "25%    -499.000060   59791.243170     2.388319    -0.760277   -51.000000   \n",
       "50%     -27.174576  241226.533900     2.691213    -0.029038     3.000000   \n",
       "75%     477.216100  547320.664300     2.869121     0.714801    52.000000   \n",
       "max     998.297367  996597.632200     2.999260     0.999992   100.000000   \n",
       "\n",
       "                X6           X7           X8           X9          X10  \\\n",
       "count  1029.000000   1029.00000  1029.000000  1029.000000  1029.000000   \n",
       "mean   -153.782162 -23648.95337    -2.565422     0.018122    -4.108330   \n",
       "std       0.000000      0.00000     0.424105     0.715733   584.648570   \n",
       "min    -153.782162 -23648.95337    -2.999260    -0.999992  -996.652132   \n",
       "25%    -153.782162 -23648.95337    -2.869121    -0.714801  -511.919891   \n",
       "50%    -153.782162 -23648.95337    -2.691213     0.029038    -6.507760   \n",
       "75%    -153.782162 -23648.95337    -2.388319     0.760277   516.046297   \n",
       "max    -153.782162 -23648.95337    -0.020395     0.999991  1000.933758   \n",
       "\n",
       "           ...               X30          X31          X32          X33  \\\n",
       "count      ...       1029.000000  1029.000000  1029.000000  1029.000000   \n",
       "mean       ...          5.612245   100.650146   -29.719145  -136.164237   \n",
       "std        ...        239.009325  2862.331915  4489.803784  2641.944700   \n",
       "min        ...       -410.000000 -4857.000000 -7715.000000 -4596.000000   \n",
       "25%        ...       -189.000000 -2457.000000 -4063.000000 -2438.000000   \n",
       "50%        ...          7.000000   123.000000   129.000000  -196.000000   \n",
       "75%        ...        214.000000  2654.000000  3890.000000  2138.000000   \n",
       "max        ...        410.000000  4862.000000  7752.000000  4577.000000   \n",
       "\n",
       "               X34          X35          X36          X37          X38  \\\n",
       "count  1029.000000  1029.000000  1029.000000  1029.000000  1029.000000   \n",
       "mean   -101.649174   -34.310010    -7.965986   -37.117590     2.499514   \n",
       "std    5153.481524  2569.220416  2933.878621  1504.903744   102.994972   \n",
       "min   -8898.000000 -4383.000000 -5133.000000 -2565.000000  -175.000000   \n",
       "25%   -4780.000000 -2311.000000 -2520.000000 -1411.000000   -85.000000   \n",
       "50%     -13.000000    20.000000   -35.000000    78.000000     6.000000   \n",
       "75%    4195.000000  2165.000000  2521.000000  1227.000000    94.000000   \n",
       "max    8866.000000  4384.000000  5123.000000  2568.000000   175.000000   \n",
       "\n",
       "                  y  \n",
       "count  1.029000e+03  \n",
       "mean  -8.035132e+18  \n",
       "std    2.593868e+20  \n",
       "min   -9.506460e+20  \n",
       "25%   -7.703790e+18  \n",
       "50%   -1.094503e+10  \n",
       "75%    5.636450e+18  \n",
       "max    9.881420e+20  \n",
       "\n",
       "[8 rows x 39 columns]"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This data does not have headers so each attribute or field is simply enumerated\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dividir los datos en train y test\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(df[df.columns[0:-1]],df[[df.columns[-1]]], train_size=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# I recommend that after manipulating data using pandas and before modelling to convert dataframes into arrays. \n",
    "# This may avoid some headaches\n",
    "X_train=np.asarray(X_train)\n",
    "X_test=np.asarray(X_test)\n",
    "Y_train=np.asarray(Y_train)\n",
    "Y_test=np.asarray(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This procedure is useful for classroom examples. For a real implementation you have to have a separete method \n",
    "# for transforming the production data so you can transform it as you get it with the fitted scaler\n",
    "## The procedure returns a standardized copy of the input data\n",
    "def normalize(X_train,X_test,Y_train,Y_test,do=True):\n",
    "\n",
    "    scale_X=preprocessing.StandardScaler()\n",
    "    scale_y=preprocessing.StandardScaler()\n",
    "    \n",
    "    train_X=np.copy(X_train)\n",
    "    train_y=np.copy(Y_train)\n",
    "    test_X=np.copy(X_test)\n",
    "    test_y=np.copy(Y_test)\n",
    "    if do:\n",
    "        scale_X.fit(train_X)\n",
    "        scale_y.fit(train_y)\n",
    "        train_X=scale_X.transform(train_X)\n",
    "        train_y=scale_y.transform(train_y)\n",
    "        test_X=scale_X.transform(test_X)\n",
    "        test_y=scale_y.transform(test_y)\n",
    "    return train_X,test_X, train_y, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ILR(X_train, Y_train, learningC, w, lam ):\n",
    "    numrows = len(X_train)    \n",
    "    numcols = len(X_train[0]) \n",
    "\n",
    "    for j in range (0,numrows):\n",
    "\n",
    "        x = np.ones(1)\n",
    "        x = np.concatenate((x,X_train[j]), axis=0)\n",
    "\n",
    "        #calculate the value of \n",
    "        res = np.dot(w,x)\n",
    "\n",
    "        #difference \n",
    "        dif = Y_train[j] - res\n",
    "\n",
    "        #print dif\n",
    "\n",
    "        for i in range (0,numcols +1 ):\n",
    "            if(i==0):\n",
    "                w[i] = w[i]+dif*learningC*x[i]\n",
    "            else:\n",
    "                w[i] = w[i]+dif*learningC*x[i]-w[i]*lam\n",
    "            \n",
    "    return w\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ILR_batch(X_train, Y_train, learningC, w, lam , numBatch, batchSize):\n",
    "    numrows = len(X_train)    \n",
    "    numcols = len(X_train[0]) \n",
    "    x_barra = np.zeros(numcols+1)\n",
    "    dif_acum = 0\n",
    "    \n",
    "\n",
    "    for j in range (0,numBatch):\n",
    "        \n",
    "        idx = np.random.randint(X_train.shape[0], size=batchSize)\n",
    "        idx\n",
    "        X_Batch = X_train[idx,:]\n",
    "        \n",
    "        \n",
    "        for k in range (0,batchSize):\n",
    "            x = np.ones(1)\n",
    "            x = np.concatenate((x,X_train[k]), axis=0)\n",
    "            x_barra = x_barra + x\n",
    "            \n",
    "            #calculate the value of \n",
    "            res = np.dot(w,x)\n",
    "            \n",
    "            #difference \n",
    "            dif = Y_train[k] - res\n",
    "            dif_acum = dif_acum + dif\n",
    "        \n",
    "        x_barra=x_barra/batchSize\n",
    "        dif_acum=dif_acum/batchSize\n",
    "\n",
    "        #actualizamos cada peso\n",
    "        for i in range (0,numcols +1 ):\n",
    "            if(i==0):\n",
    "                w[i] = w[i]+dif_acum*learningC*x_barra[i]\n",
    "            else:\n",
    "                w[i] = w[i]+dif_acum*learningC*x_barra[i]-w[i]*lam\n",
    "\n",
    "    return w\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Numero de Columnas y Renglones\n",
    "numrows = len(X_train)    # 3 rows in your example\n",
    "numcols = len(X_train[0]) # 2 columns in your example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Dividir en k subsets\n",
    "\n",
    "#inicializacion de vector de w's\n",
    "\n",
    "# returns a matrix with k subsets.\n",
    "\n",
    "def k_batch(k, X_train):\n",
    "    numrows = len(X_train)\n",
    "    numcols = len(X_train[0]) \n",
    "    \n",
    "    w = np.random.randint(k, size=numrows)\n",
    "    \n",
    "    #TamaÃ±o maximo del subset dos veces el numero de rows entre k\n",
    "    size = (numrows/k)*2\n",
    "    matrix = np.zeros((k,size,numcols))\n",
    "    \n",
    "    for s in range (0,k):\n",
    "        for j in range (0,size):\n",
    "            matrix[s][j][0] = -9999\n",
    "\n",
    "    for i in range (0,k):\n",
    "        count = 0\n",
    "        for j in range (0, numrows):\n",
    "            if(w[j]==i):\n",
    "                matrix[i][count] = X_train[j]\n",
    "                count = count + 1\n",
    "        count = 0\n",
    "    \n",
    "\n",
    "    #Guardar el nÃºmero de elementos que hay en cada subset en un arreglo de tamaÃ±o k\n",
    "    i=0\n",
    "    elements = 0\n",
    "    size = (numrows/k)*2\n",
    "    numElements = np.zeros(10)\n",
    "\n",
    "    for w in range (0,10):\n",
    "        elements = elements + i\n",
    "        i = 0\n",
    "        while( matrix[w][i][0] <> -9999 and i<=100 ):\n",
    "            i = i +1\n",
    "        numElements[w] = i\n",
    "    \n",
    "    #juntar en un arreglo k-1 subsets \n",
    "    rand = np.random.randint(10)\n",
    "    numElementsL = numrows - numElements[rand]\n",
    "    numElementsL = int(numElementsL)\n",
    "    k_1 = np.zeros((numElementsL,numcols))\n",
    "    \n",
    "    numElemento = 0\n",
    "\n",
    "    for i in range (0,10):\n",
    "        if(i <> rand):\n",
    "            for j in range(0,int(numElements[i])):\n",
    "                k_1[numElemento] = matrix[i][j]\n",
    "                numElemento = numElemento + 1\n",
    "                \n",
    "    #Separar el test subset\n",
    "    numElementsTest = int(numElements[rand])\n",
    "    k_Test = np.zeros((numElementsTest,numcols))\n",
    "    \n",
    "    for i in range(0,numElementsTest):\n",
    "        k_Test[i] = matrix[rand][i]\n",
    "\n",
    "    return matrix, k_1, k_Test\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementacion Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir los datos en train y test.\n",
    "train, test = train_test_split(df, train_size=0.75)\n",
    "\n",
    "# Convertir en arreglos\n",
    "train=np.asarray(train)\n",
    "test=np.asarray(test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separar en k subsets.\n",
    "\n",
    "Escoger aleatoriamente 1 para test y k-1 para entrenamiento.\n",
    "\n",
    "La funcion k_batch(k, dataset) Recibe dos parametros:\n",
    "    k: El nÃºmero de subsets en los que queremos dividir nuestro set de datos.\n",
    "    dataset: El set de datos se entrenamiento para hacer cross validation. \n",
    "\n",
    "Nos regresa tres arreglos. \n",
    "1. Arreglo que contiene los k subsets.\n",
    "2. Arreglo que contiene los k-1 subsets que se van usar para entrenamiento.\n",
    "3. Arreglo con el subset que se va a usar para validacion de resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_a, ksubset, ktest = k_batch(10,train)\n",
    "\n",
    "ksubset = pd.DataFrame(data=ksubset[0:,0:])\n",
    "ktest = pd.DataFrame(data=ktest[0:,0:])\n",
    "\n",
    "#Separa Columnas X and Y\n",
    "X_ksubset = ksubset[ksubset.columns[0:-1]]\n",
    "X_ktest = ktest[ktest.columns[0:-1]]\n",
    "\n",
    "#print X_ktest\n",
    "\n",
    "Y_ksubset = ksubset[ksubset.columns[-1]]\n",
    "Y_ktest = ktest[ktest.columns[-1]]\n",
    "\n",
    "\n",
    "# Transformar en arreglos para manejarlo mÃ¡s facil.\n",
    "X_ksubset=np.asarray(X_ksubset)\n",
    "X_ktest=np.asarray(X_ktest)\n",
    "Y_ksubset=np.asarray(Y_ksubset)\n",
    "Y_ktest=np.asarray(Y_ktest)\n",
    "\n",
    "#means = np.mean(ksubset, axis=0)\n",
    "#stds = np.std(ksubset, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estandarizo mis subsets de acuerdo a los valores de 'ksubset' que es el set de datos de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_ksubset = Y_ksubset.reshape(-1,1)\n",
    "Y_ktest = Y_ktest.reshape(-1,1)\n",
    "\n",
    "scalerX = preprocessing.StandardScaler()\n",
    "scalerY = preprocessing.StandardScaler()\n",
    "\n",
    "scalerX.fit(X_ksubset)\n",
    "scalerY.fit(Y_ksubset)\n",
    "\n",
    "X_ksubset=scalerX.transform(X_ksubset)\n",
    "Y_ksubset=scalerY.transform(Y_ksubset)\n",
    "X_ktest=scalerX.transform(X_ktest)\n",
    "Y_ktest=scalerY.transform(Y_ktest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tenemos que hacer pruebas para diferentes valores de lambda y verificar cual parametro es el que me genera un menor error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CASO 1\n",
      "Lambda =  0.01\n",
      "MSE =  3.69188830467 \n",
      "\n",
      "CASO 2\n",
      "Lambda =  0.02\n",
      "MSE =  3.89523213289 \n",
      "\n",
      "CASO 3\n",
      "Lambda =  0.03\n",
      "MSE =  4.46480083584 \n",
      "\n",
      "CASO 4\n",
      "Lambda =  0.025\n",
      "MSE =  4.12108641935 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ILR(X_train, Y_train, learningC, w, lam ):\n",
    "\n",
    "lam1 = .01\n",
    "lam2 = .02\n",
    "lam3 =  .03\n",
    "lam4 =  .025\n",
    "\n",
    "#lam1\n",
    "w = np.ones(numcols+1)\n",
    "for j in range (0,10):\n",
    "    w = ILR(X_ksubset, Y_ksubset, lam1, w, 0.0)\n",
    "    X_mod = np.c_[ X_ktest, np.ones(X_ktest.shape[0]) ]\n",
    "    Y_pred = np.dot(w,X_mod.T)\n",
    "\n",
    "print \"CASO 1\"\n",
    "print \"Lambda = \", lam1\n",
    "print \"MSE = \" , mean_squared_error(Y_ktest, Y_pred) , \"\\n\"\n",
    "\n",
    "#lam2\n",
    "w = np.ones(numcols+1)\n",
    "for j in range (0,10):\n",
    "    w = ILR(X_ksubset, Y_ksubset, lam2, w, 0.0)\n",
    "    X_mod = np.c_[ X_ktest, np.ones(X_ktest.shape[0]) ]\n",
    "    Y_pred = np.dot(w,X_mod.T)\n",
    "\n",
    "print \"CASO 2\"\n",
    "print \"Lambda = \", lam2\n",
    "print \"MSE = \" , mean_squared_error(Y_ktest, Y_pred) , \"\\n\"\n",
    "\n",
    "#lam3\n",
    "w = np.ones(numcols+1)\n",
    "for j in range (0,10):\n",
    "    w = ILR(X_ksubset, Y_ksubset, lam3, w, 0.0)\n",
    "    X_mod = np.c_[ X_ktest, np.ones(X_ktest.shape[0]) ]\n",
    "    Y_pred = np.dot(w,X_mod.T)\n",
    "\n",
    "print \"CASO 3\"\n",
    "print \"Lambda = \", lam3\n",
    "print \"MSE = \" , mean_squared_error(Y_ktest, Y_pred) , \"\\n\"\n",
    "\n",
    "#lam4\n",
    "w = np.ones(numcols+1)\n",
    "for j in range (0,10):\n",
    "    w = ILR(X_ksubset, Y_ksubset, lam4, w, 0.0)\n",
    "    X_mod = np.c_[ X_ktest, np.ones(X_ktest.shape[0]) ]\n",
    "    Y_pred = np.dot(w,X_mod.T)\n",
    "\n",
    "print \"CASO 4\"\n",
    "print \"Lambda = \", lam4\n",
    "print \"MSE = \" , mean_squared_error(Y_ktest, Y_pred) , \"\\n\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
